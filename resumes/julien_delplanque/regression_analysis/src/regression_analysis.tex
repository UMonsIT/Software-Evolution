\documentclass[a4paper,11pt]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[francais]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{fullpage}
\usepackage{graphicx}

\renewcommand{\thesection}{\arabic{section}}

\newtheorem{definition}{Definition}

\title{Analysing Software Evolution Metrics - Regression analysis: Summary}
\author{Julien Delplanque}
\begin{document}
\maketitle
\newpage

\section{Regression Analysis}
Given data points, regression analysis can be used to obtain functions that fit
them. This technique is widely used for \textbf{prediction} or
\textbf{forecasting} to fit a predictive model to an observed data set. Once
the function is obtained, different measures of \textbf{goodness of fit} can be
done (e.g. $R^2$, MMRE, MMER, etc\dots). A visualisation can then confirm the
results of the analysis.

\begin{definition}[Regression analysis]
Any approach to modeling the relationship between one or more variables, denoted
$y_i$, and one or more variables denoted $x_i$, such that the model depends on
the unknown parameters to be estimated from the data.
\end{definition}

\begin{definition}[Parametric regression method]
Finite number of unknown parameters that are estimated from the data points
(e.g. linear regression - he regression function $f(x)=ax+b$ has 2 parameters
$a$ and $b$).
\end{definition}

Regression methods must be used with caution as \underline{they can easily give
misleading results}. Regression analysis can be used to compute trend lines,
that show how data changes over time.

\section{Goodness of fit}
The coefficient of determination $R^2$ provides a measure of how well future
outcomes are likely to be predicted by the statistical model and is the
proportion of variability in a data set that is accounted for by the statistical
model. In regression analysis, it measures how well the regression model
approximates the real data points. A value of 1 indicates that the regression
model perfectly fits the data.

\begin{definition}[$R^2$]
Let $y_i$ be the observed data values,\\
Let $f_i$ be the values modeled by the regression function,\\
Let $Y$ be the average of all observed data values $y_i$,\\
Let $\text{SS}_{\text{tot}} = \sum\limits_{i}{(y_i-Y)^2}$,\\
Let $\text{SS}_{\text{err}} = \sum\limits_{i}{(y_i-f_i)^2}$,\\

Then,
\begin{center}
$R^2 = 1 - \frac{SS_{\text{err}}}{SS_{\text{tot}}}$
\end{center}
\end{definition}

\textbf{Attention! Only compare statistical models that are comparable!} For
example, a model with more parameters typically leads to a better fit but this
goodness of fit cannot be compared with regression models that use less
parameters $\leadsto$ $R^2$ alone cannot be used as a meaningful comparison of
models with different numbers of independent variables. Solution: adjusted
$R^2$: $\overline{R}^2$.

\begin{definition}[$\overline{R}^2$]
Let $p$ be the number of regressors (explanatory variables excluding constant
term) in the model and $n$ the number of data points,\\

Then,
\begin{center}
$\overline{R}^2 = 1 - (1 - R^2)\frac{n-1}{n-p-1} = R^2 - (1 - R^2)
\frac{p}{n-p-1}$
\end{center}
\end{definition}

\begin{definition}[Magnitude of Relative Error]
$\text{MRE}_i = |y_i-f_i| / |y_i|$
\end{definition}

\begin{definition}[Mean Magnitude of Relative Error]
Average of all $\text{MRE}_i$ values.\\

A value of 0 indicates that the regression line perfectly fits the data.\\
According to Foss et al. (IEEE TSE, Nov. 2003), MMRE does not always select the
best model among competing prediction models.
\end{definition}

\begin{definition}[MER]
Slight variant of MRE: $\text{MER}_i = |y_i-f_i| / |f_i|$
\end{definition}

\begin{definition}[MMER]
Average of all $\text{MER}_i$ values.\\

A value of 0 indicates that the regression line perfectly fits the data.\\
According to Foss et al. (IEEE TSE, Nov. 2003), MMER is slightly better than
MMRE.
\end{definition}

\begin{definition}[Akaike’s Information Criterion]
AIC is a test for selecting the best model for a given data set. Competing
models may be ranked according to their AIC, with the one having the lowest AIC
being the best.\\
\end{definition}

\section{Correlation analysis}
The goal of correlation analysis is to find a possible relationship between two
sets of data (e.g. a relation between the size (measured in lines of code) and
the complexity (measured in number of conditional statements) of software
components). To do that, statistical correlation coefficients are used (e.g.
Pearson correlation coefficient, Spearman’s rank correlation coefficient,
Kendall’s rank correlation coefficient, etc\dots).

\begin{definition}[Pearson correlation coefficient]
A measure of the linear (dependence) between two variables $X$ and $Y$. It
returns a value between $-1$ and $1$ where:
\begin{itemize}
\item $1$ is total positive correlation
\item $0$ is no correlation
\item $-1$ is total negative correlation
\end{itemize}
\end{definition}

The Pearson correlation coefficient has the following problems: it is sensitive
to data distribution and it is not robust in presence of outliers.

\begin{definition}[Spearman’s rank correlation coefficient]
A non-parametric (does not assume any particular statistical distribution such
as normal distribution) measure of statistical dependence between two variables.
It assesses how well the relationship between two variables can be described
using a monotonic function.\\

If there are no repeated data values, a perfect Spearman correlation of $+1$ or
$-1$ occurs when each of the variables is a perfect monotone function of the
other.
\end{definition}
\end{document}
